
[![(latest) release | GitHub](https://img.shields.io/github/release/pocokhc/simple_distributed_rl.svg?logo=github&style=popout)](https://github.com/pocokhc/simple_distributed_rl/releases/latest)

# Simple Distributed Reinforcement Learning FrameWork (SRL)

I am creating a simple distributed reinforcement learning framework.

It has the following features:

+ Support for distributed reinforcement learning
+ Automatic adjustment of the interface between the environment and the algorithm
+ Support for Gym/Gymnasium environments
+ Provides customizable environment classes
+ Provides customizable reinforcement learning algorithm classes
+ Provides well-known reinforcement learning algorithms
+ (Support for new algorithms)

**Document**

<https://pocokhc.github.io/simple_distributed_rl/>


**Algorithm explanation article (Qiita)**

The article will be explained in Japanese.

<https://qiita.com/pocokhc/items/a2f1ba993c79fdbd4b4d>


# 1. Installation

You can install it from PyPI.

``` bash
# Install only the core functionality
pip install srl-framework

# Install with major extensions and auxiliary libraries (TensorFlow and PyTorch not included)
pip install srl-framework[full]
```

TensorFlow and PyTorch are not included, so please install them separately.

+ TensorFlow
  + <https://www.tensorflow.org/install>
  + tensorflow-probability[tf]
+ PyTorch
  + <https://pytorch.org/get-started/locally/>

For details on the installed libraries or how to use the framework without installation, please refer to the following documentation:

<https://pocokhc.github.io/simple_distributed_rl/pages/install.html>


# 2. Usage

Here's a simple usage example.

``` python
import srl
from srl.algorithms import ql  # Importing ql algorithms


def main():
    # Creating a runner
    runner = srl.Runner("Grid", ql.Config())

    # train
    runner.train(timeout=10)

    # Evaluation of training results
    rewards = runner.evaluate()
    print(f"evaluate episodes: {rewards}")

    # Visualization example
    runner.animation_save_gif("Grid.gif")


if __name__ == "__main__":
    main()

```

The image generated by animation_save_gif is below.

![Grid.gif](Grid.gif)

For details on how to use it, please refer to the following document.

<https://pocokhc.github.io/simple_distributed_rl/pages/howtouse.html>


# 3. Custom environment and algorithms

Please refer to the following documents for creating original environments and algorithms.  
(The contents are in Japanese)

+ [Making a Custom environment](https://pocokhc.github.io/simple_distributed_rl/pages/custom_env.html)
+ [Ways to Interact with External Environments](./examples/external_env/)
+ [Making a Custom algorithm](https://pocokhc.github.io/simple_distributed_rl/pages/custom_algorithm.html)


# 4. Algorithms
## ModelFree
### ValueBase

|Algorithm |Observation|Action  |Tensorflow|Torch|ProgressRate||
|----------|-----------|--------|----------|-----|------------|---|
|QL        |Discrete   |Discrete|-         |- |100%|Basic Q Learning|
|DQN       |Continuous |Discrete|✔        |✔|100%||
|C51       |Continuous |Discrete|✔        |- |99%|CategoricalDQN|
|Rainbow   |Continuous |Discrete|✔        |✔|100%||
|R2D2      |Continuous |Discrete|✔        |- |100%||
|Agent57   |Continuous |Discrete|✔        |✔|100%||
|SND       |Continuous |Discrete|✔        |-|100%||
|Go-Explore|Continuous |Discrete|✔        |- |100%|DQN base, R2D3 memory base|

### PolicyBase/ActorCritic

|Algorithm     |Observation|Action    |Tensorflow|Torch|ProgressRate||
|--------------|-----------|----------|----------|-----|---|---|
|VanillaPolicy |Discrete   |Both      |-         |-    |100%||
|A3C/A2C       |-          |-         |-         |-    |-   ||
|TRPO          |-          |-         |-         |-    |-   ||
|PPO           |Continuous |Both      |✔        |-    |100%||
|DDPG/TD3      |Continuous |Continuous|✔        |-    |100%||
|SAC           |Continuous |Both      |✔        |-    |100%||

## AlphaSeries

|Algorithm  |Observation|Action  |Tensorflow|Torch|ProgressRate||
|-----------|-----------|--------|----------|-----|---|---|
|MCTS       |Discrete   |Discrete|-         |-    |100%|MDP base|
|AlphaZero  |Image      |Discrete|✔        |-    |100%|MDP base|
|MuZero     |Image      |Discrete|✔        |-    |100%|MDP base|
|StochasticMuZero|Image |Discrete|✔        |-    |100%|MDP base|

## ModelBase

|Algorithm  |Observation|Action     |Framework|ProgressRate|
|-----------|-----------|-----------|---------|----|
|DynaQ      |Discrete   |Discrete   |-        |100%|

## WorldModels

|Algorithm  |Observation|Action     |Tensorflow|Torch|ProgressRate|
|-----------|-----------|-----------|----------|-----|---|
|WorldModels|Continuous |Discrete   |✔        |-    |100%|
|PlaNet     |Continuous |Discrete   |✔(+tensorflow-probability)|-|100%|
|Dreamer    |Continuous |Both       |-|-|merge DreamerV3|
|DreamerV2  |Continuous |Both       |-|-|merge DreamerV3|
|DreamerV3  |Continuous |Both       |✔(+tensorflow-probability)|-|100%|
|DIAMOND    |Image      |Discrete   |✔|-|100%|

## Offline

|Algorithm  |Observation|Action     |Framework|ProgressRate|
|-----------|-----------|-----------|----------|----|
|CQL        |Discrete   |Discrete   |          |  0%|

## Original

|Algorithm     |Observation|Action  |Type     |Tensorflow|Torch|ProgressRate||
|--------------|-----------|--------|---------|----------|-----|---|---|
|QL_agent57    |Discrete   |Discrete|ValueBase|-         |-    |80%|QL + Agent57|
|Agent57_light |Continuous |Discrete|ValueBase|✔        |✔   |100%|Agent57 - (LSTM,MultiStep)|
|SearchDynaQ   |Discrete   |Discrete|ModelBase|-         |-    |100%|original|
|GoDynaQ       |Discrete   |Discrete|ModelBase|-         |-    |99%|original|
|GoDQN         |Continuous |Discrete|ValueBase|✔        |-    |90%|original|


# 5. Online Distributed Learning

For information on distributed learning over a network, please refer to the following documents.
(The contents are in Japanese)

+ [Distributed Learning (Online)](https://pocokhc.github.io/simple_distributed_rl/pages/distributed.html)

For information on linking with cloud services, please refer to the Qiita article.
(The contents are in Japanese)

+ [Distributed Reinforcement Learning Using Cloud Services (Free Edition)](https://qiita.com/pocokhc/items/f7a32ee6c62cba54d6ab)
+ [Distributed Reinforcement Learning Using Cloud Services (Kubernetes)](https://qiita.com/pocokhc/items/56c930e1e401ce156141)
+ [Distributed Reinforcement Learning Using Cloud Services (GKE/Paid Edition)](https://qiita.com/pocokhc/items/e08aab0fe56566ab9407)


# 6. Framework Overview

![overview-sequence.drawio.png](diagrams/overview-sequence.drawio.png)

+ Simplified pseudo code

For implementation details, see 'Making a Custom algorithm'.

``` python
# Initialize learning units
env.setup()
worker.setup()
trainer.setup()

for episode in range(N)
  # 1 episode initializing phase
  env.reset()
  worker.reset()

  # 1 episode loop
  while not env.done:
      # get action
      action = worker.policy()

      # render
      env.render()
      worker.render()

      # Run 1 step of the environment
      env.step(action)
      worker.on_step()

      # Train phase
      trainer.train()

  # Drawing after the end of one episode
  env.render()

# End of learning unit
env.teardown()
worker.teardown()
trainer.teardown()
```


# 7. Development environment

Look [dockers folder](./dockers/)

+ PC1
  + windows11
  + CPUx1: Core i7-8700 3.2GHz
  + GPUx1: NVIDIA GeForce GTX 1060 3GB
  + memory 48GB
+ PC2
  + windows11
  + CPUx1: Core i9-12900 2.4GHz
  + GPUx1: NVIDIA GeForce RTX 3060 12GB
  + memory 32GB
