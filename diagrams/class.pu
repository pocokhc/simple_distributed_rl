@startuml class

abstract gym.Env {
    action_space : gym.spaces.Space
    observation_space : gym.spaces.Space
    reset()
    step(action)
    render()
}


enum RLActionType {
    ANY
    DISCRETE
    CONTINUOUS
}

enum RLObservationType {
    ANY
    DISCRETE
    CONTINUOUS
}

enum EnvObservationType {
    DISCRETE
    CONTINUOUS
    GRAY_2ch
    GRAY_3ch
    COLOR
    SHAPE2
    SHAPE3
}

abstract EnvBase {
    action_space : gym.spaces.Space
    observation_space : gym.spaces.Space
    reset()
    step(action)
    render()
    - action_type : EnvActionType
    - observation_type : EnvObservationType
    - max_episode_steps : int
    - player_num : int
    - get_invalid_actions()
    - sample()
    - backup()
    - restore()
} 


abstract RLConfig {
    {static} getName() : str
    action_type : RLActionType
    observation_type : RLObservationType
    assert_params()
    set_config_by_env(EnvForRL)
}

abstract RLRemoteMemory {
    config : RLConfig
    backup()
    restore()
}

abstract RLParameter {
    config : RLConfig
    restore()
    backup()
}

abstract RLTrainer {
    config : RLConfig
    parameter : RLParameter
    remote_memory : RLRemoteMemory
    train()
    get_train_count()
}

abstract RLWorker {
    config : RLConfig
    parameter : RLParameter
    remote_memory : RLRemoteMemory
    worker_id : int
    on_reset()
    policy()
    on_step()
    render()
    observation_encode()
    action_decode()
}

RLConfig .. RLActionType
RLConfig .. RLObservationType


EnvBase <|-- gym.Env
EnvBase .. EnvObservationType

RLWorker -- EnvBase

note "Absorbs the difference with the environment." as RLWorkerMemo
RLWorker .. RLWorkerMemo

RLConfig -- RLWorker
RLConfig -- RLTrainer
RLConfig -- RLParameter
RLConfig -- RLRemoteMemory
RLParameter -- RLWorker
RLParameter -- RLTrainer
RLRemoteMemory -- RLWorker
RLRemoteMemory -- RLTrainer


@enduml